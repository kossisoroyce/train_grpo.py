{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ GRPO Trainer Super Notebook\n",
    "\n",
    "**Advanced GRPO/GSPO Training for Math & Reasoning Tasks**\n",
    "\n",
    "This notebook provides a complete, production-ready pipeline for training LLMs using:\n",
    "- **GRPO** (Group Relative Policy Optimization)\n",
    "- **GSPO** (dr_grpo variant with sequence-level importance sampling)\n",
    "- **Multi-dataset support** (GSM8K, MATH, custom)\n",
    "- **Memory-efficient training** (4-bit quantization, 8-bit optimizers)\n",
    "- **Configurable reward functions**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Configuration](#config)\n",
    "3. [Dataset Preparation](#data)\n",
    "4. [Model Loading](#model)\n",
    "5. [Reward Functions](#rewards)\n",
    "6. [Training](#train)\n",
    "7. [Evaluation](#eval)\n",
    "8. [Inference](#inference)\n",
    "9. [Export & Save](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "## 1. Setup & Installation\n",
    "\n",
    "Run this cell to install all dependencies. Works on Google Colab (free T4 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install Dependencies { display-mode: \"form\" }\n",
    "# @markdown Check the boxes for optional features:\n",
    "install_flash_attn = False  # @param {type:\"boolean\"}\n",
    "install_deepspeed = False  # @param {type:\"boolean\"}\n",
    "use_unsloth = True  # @param {type:\"boolean\"}\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run(cmd):\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "# Core dependencies\n",
    "run(\"pip install -q torch transformers>=4.40.0 trl>=0.8.0 peft>=0.10.0\")\n",
    "run(\"pip install -q datasets accelerate bitsandbytes wandb\")\n",
    "run(\"pip install -q rich typer pyyaml omegaconf\")\n",
    "\n",
    "if use_unsloth:\n",
    "    run(\"pip install -q unsloth\")\n",
    "    \n",
    "if install_flash_attn:\n",
    "    run(\"pip install flash-attn --no-build-isolation -q\")\n",
    "    \n",
    "if install_deepspeed:\n",
    "    run(\"pip install deepspeed -q\")\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU\n",
    "print(f\"üñ•Ô∏è PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"config\"></a>\n",
    "## 2. Configuration\n",
    "\n",
    "Configure your training run. All settings are in one place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Configuration { display-mode: \"form\" }\n",
    "\n",
    "# === MODEL ===\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # @param {type:\"string\"}\n",
    "LOAD_IN_4BIT = True  # @param {type:\"boolean\"}\n",
    "USE_FLASH_ATTN = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# === LORA ===\n",
    "LORA_ENABLED = True  # @param {type:\"boolean\"}\n",
    "LORA_R = 16  # @param {type:\"integer\"}\n",
    "LORA_ALPHA = 64  # @param {type:\"integer\"}\n",
    "LORA_DROPOUT = 0.05  # @param {type:\"number\"}\n",
    "\n",
    "# === DATASET ===\n",
    "DATASET_NAME = \"gsm8k\"  # @param [\"gsm8k\", \"math\", \"custom\"]\n",
    "USE_ONE_SHOT = True  # @param {type:\"boolean\"}\n",
    "MAX_SAMPLES = None  # @param {type:\"integer\"}\n",
    "\n",
    "# === TRAINING ===\n",
    "OUTPUT_DIR = \"outputs/grpo-run\"  # @param {type:\"string\"}\n",
    "RUN_NAME = \"grpo-gsm8k\"  # @param {type:\"string\"}\n",
    "LEARNING_RATE = 5e-6  # @param {type:\"number\"}\n",
    "BATCH_SIZE = 2  # @param {type:\"integer\"}\n",
    "GRAD_ACCUM_STEPS = 2  # @param {type:\"integer\"}\n",
    "NUM_GENERATIONS = 8  # @param {type:\"integer\"}\n",
    "NUM_EPOCHS = 1  # @param {type:\"number\"}\n",
    "MAX_STEPS = -1  # @param {type:\"integer\"}\n",
    "\n",
    "# === GSPO OPTIONS ===\n",
    "USE_GSPO = False  # @param {type:\"boolean\"}\n",
    "LOSS_TYPE = \"grpo\"  # @param [\"grpo\", \"dr_grpo\", \"ipo\", \"simpo\"]\n",
    "IMPORTANCE_SAMPLING = \"token\"  # @param [\"token\", \"sequence\"]\n",
    "\n",
    "# === OPTIMIZER ===\n",
    "USE_8BIT_OPTIMIZER = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === REWARD WEIGHTS ===\n",
    "CORRECTNESS_WEIGHT = 2.0  # @param {type:\"number\"}\n",
    "FORMAT_WEIGHT = 0.5  # @param {type:\"number\"}\n",
    "INTEGER_WEIGHT = 0.5  # @param {type:\"number\"}\n",
    "XML_COUNT_WEIGHT = 0.5  # @param {type:\"number\"}\n",
    "\n",
    "# === CUSTOM DELIMITERS ===\n",
    "USE_CUSTOM_DELIMITERS = False  # @param {type:\"boolean\"}\n",
    "REASONING_START = \"<REASONING>\"  # @param {type:\"string\"}\n",
    "REASONING_END = \"</REASONING>\"  # @param {type:\"string\"}\n",
    "ANSWER_START = \"<SOLUTION>\"  # @param {type:\"string\"}\n",
    "ANSWER_END = \"</SOLUTION>\"  # @param {type:\"string\"}\n",
    "\n",
    "# === LOGGING ===\n",
    "REPORT_TO = \"none\"  # @param [\"none\", \"wandb\", \"tensorboard\"]\n",
    "LOGGING_STEPS = 1  # @param {type:\"integer\"}\n",
    "SAVE_STEPS = 100  # @param {type:\"integer\"}\n",
    "\n",
    "# Apply GSPO settings\n",
    "if USE_GSPO:\n",
    "    LOSS_TYPE = \"dr_grpo\"\n",
    "    IMPORTANCE_SAMPLING = \"sequence\"\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   LoRA: {'Enabled' if LORA_ENABLED else 'Disabled'} (r={LORA_R})\")\n",
    "print(f\"   4-bit: {'Yes' if LOAD_IN_4BIT else 'No'}\")\n",
    "print(f\"   Loss: {LOSS_TYPE}\")\n",
    "print(f\"   Optimizer: {'8-bit AdamW' if USE_8BIT_OPTIMIZER else 'AdamW'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "## 3. Dataset Preparation\n",
    "\n",
    "Load and format the dataset with chat templates and one-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts based on delimiter choice\n",
    "if USE_CUSTOM_DELIMITERS:\n",
    "    SYSTEM_PROMPT = f\"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "{REASONING_START}\n",
    "...\n",
    "{REASONING_END}\n",
    "{ANSWER_START}\n",
    "...\n",
    "{ANSWER_END}\n",
    "\"\"\"\n",
    "    COT_FORMAT = f\"\"\"\\\n",
    "{REASONING_START}\n",
    "{{reasoning}}\n",
    "{REASONING_END}\n",
    "{ANSWER_START}\n",
    "{{answer}}\n",
    "{ANSWER_END}\n",
    "\"\"\"\n",
    "else:\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "    COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "    REASONING_START, REASONING_END = \"<reasoning>\", \"</reasoning>\"\n",
    "    ANSWER_START, ANSWER_END = \"<answer>\", \"</answer>\"\n",
    "\n",
    "# One-shot examples\n",
    "ONE_SHOT_EXAMPLES = {\n",
    "    \"gsm8k\": {\n",
    "        \"question\": \"What is the largest single-digit prime number?\",\n",
    "        \"reasoning\": \"9 is divisible by 3 and 8 is divisible by 2, but 7 is prime.\",\n",
    "        \"answer\": \"7\"\n",
    "    },\n",
    "    \"math\": {\n",
    "        \"question\": \"Find the value of x if 2x + 5 = 13.\",\n",
    "        \"reasoning\": \"Subtract 5 from both sides: 2x = 8. Divide by 2: x = 4.\",\n",
    "        \"answer\": \"4\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìù System Prompt Preview:\")\n",
    "print(SYSTEM_PROMPT[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"Extract answer from GSM8K format (#### answer)\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def extract_boxed_answer(text: str) -> str | None:\n",
    "    \"\"\"Extract answer from MATH format (\\\\boxed{answer})\"\"\"\n",
    "    pattern = r\"\\\\boxed\\{([^}]+)\\}\"\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def load_and_prepare_dataset(dataset_name: str, use_one_shot: bool = True, max_samples: int = None):\n",
    "    \"\"\"Load and prepare dataset with chat formatting.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading dataset: {dataset_name}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if dataset_name == \"gsm8k\":\n",
    "        data = load_dataset(\"openai/gsm8k\", \"main\")[\"train\"]\n",
    "        question_field = \"question\"\n",
    "        answer_fn = lambda x: extract_hash_answer(x[\"answer\"])\n",
    "    elif dataset_name == \"math\":\n",
    "        data = load_dataset(\"hendrycks/competition_math\")[\"train\"]\n",
    "        question_field = \"problem\"\n",
    "        answer_fn = lambda x: extract_boxed_answer(x[\"solution\"]) or x.get(\"answer\", \"\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    def format_example(x):\n",
    "        prompt = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        \n",
    "        # Add one-shot example\n",
    "        if use_one_shot and dataset_name in ONE_SHOT_EXAMPLES:\n",
    "            ex = ONE_SHOT_EXAMPLES[dataset_name]\n",
    "            prompt.extend([\n",
    "                {\"role\": \"user\", \"content\": ex[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": COT_FORMAT.format(\n",
    "                    reasoning=ex[\"reasoning\"],\n",
    "                    answer=ex[\"answer\"]\n",
    "                )}\n",
    "            ])\n",
    "        \n",
    "        prompt.append({\"role\": \"user\", \"content\": x[question_field]})\n",
    "        return {\"prompt\": prompt, \"answer\": answer_fn(x)}\n",
    "    \n",
    "    # Format dataset\n",
    "    formatted = data.map(format_example)\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples and len(formatted) > max_samples:\n",
    "        formatted = formatted.shuffle(seed=42).select(range(max_samples))\n",
    "    \n",
    "    logger.info(f\"Dataset size: {len(formatted)} samples\")\n",
    "    return formatted\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_and_prepare_dataset(DATASET_NAME, USE_ONE_SHOT, MAX_SAMPLES)\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nüìù Sample prompt structure:\")\n",
    "print(json.dumps(dataset[0][\"prompt\"][:2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "## 4. Model Loading\n",
    "\n",
    "Load the model with optional 4-bit quantization and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, load_in_4bit=True, use_flash_attn=False):\n",
    "    \"\"\"Load model with quantization and attention optimizations.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Quantization config\n",
    "    quant_config = None\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "    \n",
    "    # Model kwargs\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"use_cache\": False,\n",
    "    }\n",
    "    \n",
    "    if quant_config:\n",
    "        model_kwargs[\"quantization_config\"] = quant_config\n",
    "    \n",
    "    if use_flash_attn:\n",
    "        model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "    \n",
    "    # Prepare for training if quantized\n",
    "    if load_in_4bit:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Model loaded: {total_params/1e9:.2f}B parameters\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME, LOAD_IN_4BIT, USE_FLASH_ATTN)\n",
    "print(f\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "if LORA_ENABLED:\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    print(f\"‚úÖ LoRA config ready (r={LORA_R}, alpha={LORA_ALPHA})\")\n",
    "else:\n",
    "    peft_config = None\n",
    "    print(\"‚ö†Ô∏è LoRA disabled - full fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rewards\"></a>\n",
    "## 5. Reward Functions\n",
    "\n",
    "Define reward functions to guide GRPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"Extract answer from XML-formatted response.\"\"\"\n",
    "    try:\n",
    "        answer = text.split(ANSWER_START)[-1].split(ANSWER_END)[0].strip()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "# === REWARD FUNCTIONS ===\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for correct answers.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted = [extract_xml_answer(r) for r in responses]\n",
    "    \n",
    "    # Log first example\n",
    "    q = prompts[0][-1]['content']\n",
    "    logger.info(f\"Q: {q[:100]}... | A: {answer[0]} | Got: {extracted[0]}\")\n",
    "    \n",
    "    return [CORRECTNESS_WEIGHT if r == a else 0.0 for r, a in zip(extracted, answer)]\n",
    "\n",
    "def integer_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for numeric answers.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted = [extract_xml_answer(r) for r in responses]\n",
    "    return [INTEGER_WEIGHT if r.lstrip('-').isdigit() else 0.0 for r in extracted]\n",
    "\n",
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for correct formatting.\"\"\"\n",
    "    pattern = f\"{re.escape(REASONING_START)}.*?{re.escape(REASONING_END)}.*?{re.escape(ANSWER_START)}.*?{re.escape(ANSWER_END)}\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    return [FORMAT_WEIGHT if re.search(pattern, r, re.DOTALL) else 0.0 for r in responses]\n",
    "\n",
    "def xml_count_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward based on XML tag structure.\"\"\"\n",
    "    def count_tags(text):\n",
    "        score = 0.0\n",
    "        if text.count(f\"{REASONING_START}\\n\") == 1:\n",
    "            score += 0.125\n",
    "        if text.count(f\"\\n{REASONING_END}\\n\") == 1:\n",
    "            score += 0.125\n",
    "        if text.count(f\"\\n{ANSWER_START}\\n\") == 1:\n",
    "            score += 0.125\n",
    "        if text.count(f\"\\n{ANSWER_END}\") == 1:\n",
    "            score += 0.125\n",
    "            # Penalize content after closing tag\n",
    "            extra = text.split(f\"\\n{ANSWER_END}\")[-1]\n",
    "            score -= len(extra) * 0.001\n",
    "        return max(score, 0.0) * XML_COUNT_WEIGHT\n",
    "    \n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    return [count_tags(r) for r in responses]\n",
    "\n",
    "def gibberish_penalty_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Penalize gibberish outputs (VLM fix).\"\"\"\n",
    "    patterns = [\"addCriterion\", \"\\n\\n\\n\\n\", \"................\"]\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    \n",
    "    rewards = []\n",
    "    for r in responses:\n",
    "        if len(r) == 0:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        cleaned = r\n",
    "        for p in patterns:\n",
    "            cleaned = cleaned.replace(p, \"\")\n",
    "        if (len(r) - len(cleaned)) / len(r) >= 0.5:\n",
    "            rewards.append(-2.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "# Combine reward functions\n",
    "reward_functions = [\n",
    "    correctness_reward_func,\n",
    "    format_reward_func,\n",
    "    integer_reward_func,\n",
    "]\n",
    "\n",
    "if XML_COUNT_WEIGHT > 0:\n",
    "    reward_functions.append(xml_count_reward_func)\n",
    "\n",
    "print(f\"‚úÖ {len(reward_functions)} reward functions configured\")\n",
    "print(f\"   Weights: correctness={CORRECTNESS_WEIGHT}, format={FORMAT_WEIGHT}, integer={INTEGER_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train\"></a>\n",
    "## 6. Training\n",
    "\n",
    "Configure and run GRPO/GSPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    run_name=RUN_NAME,\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.1,\n",
    "    optim=\"adamw_8bit\" if USE_8BIT_OPTIMIZER else \"adamw_torch\",\n",
    "    \n",
    "    # Batch sizes\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    \n",
    "    # GRPO specific\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=786,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    max_steps=MAX_STEPS if MAX_STEPS > 0 else -1,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=REPORT_TO,\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "\n",
    "# Add GSPO options if enabled\n",
    "if USE_GSPO or LOSS_TYPE != \"grpo\":\n",
    "    training_args.loss_type = LOSS_TYPE\n",
    "    training_args.importance_sampling_level = IMPORTANCE_SAMPLING\n",
    "    training_args.mask_truncated_completions = False\n",
    "    print(f\"üîß GSPO enabled: loss={LOSS_TYPE}, sampling={IMPORTANCE_SAMPLING}\")\n",
    "\n",
    "print(f\"‚úÖ Training config ready\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS * NUM_GENERATIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=reward_functions,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(f\"   Ready to train on {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Start Training { display-mode: \"form\" }\n",
    "# @markdown Click to start training. Watch the reward column increase!\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"=\"*50)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eval\"></a>\n",
    "## 7. Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset_name=\"gsm8k\", num_samples=100):\n",
    "    \"\"\"Evaluate model accuracy on test set.\"\"\"\n",
    "    \n",
    "    # Load test set\n",
    "    if dataset_name == \"gsm8k\":\n",
    "        test_data = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]\n",
    "        q_field, a_fn = \"question\", lambda x: extract_hash_answer(x[\"answer\"])\n",
    "    else:\n",
    "        test_data = load_dataset(\"hendrycks/competition_math\")[\"test\"]\n",
    "        q_field, a_fn = \"problem\", lambda x: extract_boxed_answer(x[\"solution\"])\n",
    "    \n",
    "    if num_samples:\n",
    "        test_data = test_data.select(range(min(num_samples, len(test_data))))\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    for example in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        question = example[q_field]\n",
    "        gold = a_fn(example)\n",
    "        \n",
    "        # Generate response\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        extracted = extract_xml_answer(response)\n",
    "        \n",
    "        if extracted == gold:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation\n",
    "# accuracy = evaluate_model(model, tokenizer, DATASET_NAME, num_samples=50)\n",
    "print(\"üí° Uncomment the line above to run evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"inference\"></a>\n",
    "## 8. Inference\n",
    "\n",
    "Test the trained model with custom prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str, max_tokens: int = 512):\n",
    "    \"\"\"Generate a response for a given question.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test inference\n",
    "test_question = \"If a train travels at 60 mph for 2.5 hours, how far does it travel?\"\n",
    "print(f\"‚ùì Question: {test_question}\")\n",
    "print(f\"\\nü§ñ Response:\")\n",
    "print(generate_response(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"save\"></a>\n",
    "## 9. Export & Save\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Model { display-mode: \"form\" }\n",
    "save_path = \"outputs/final_model\"  # @param {type:\"string\"}\n",
    "push_to_hub = False  # @param {type:\"boolean\"}\n",
    "hub_model_id = \"your-username/model-name\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"üíæ Saving model to {save_path}...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Save config\n",
    "config_dict = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"loss_type\": LOSS_TYPE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "with open(f\"{save_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved!\")\n",
    "\n",
    "# Push to Hub\n",
    "if push_to_hub:\n",
    "    print(f\"üì§ Pushing to Hub: {hub_model_id}\")\n",
    "    model.push_to_hub(hub_model_id)\n",
    "    tokenizer.push_to_hub(hub_model_id)\n",
    "    print(\"‚úÖ Pushed to Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Saved Model { display-mode: \"form\" }\n",
    "# @markdown Use this to load a previously saved model\n",
    "\n",
    "load_path = \"outputs/final_model\"  # @param {type:\"string\"}\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, load_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "\n",
    "print(f\"‚úÖ Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Done!\n",
    "\n",
    "You've successfully trained a model using GRPO! Next steps:\n",
    "\n",
    "1. **Experiment** with different reward weights\n",
    "2. **Try GSPO** (`USE_GSPO = True`) for more stable training\n",
    "3. **Scale up** with larger models and DeepSpeed\n",
    "4. **Evaluate** on different test sets\n",
    "\n",
    "For more info, see the [GRPO Trainer documentation](https://github.com/kossisoroyce/grpo-trainer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
